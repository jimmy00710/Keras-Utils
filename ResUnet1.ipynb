{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras import losses\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Dropout, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers \n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from matplotlib.pyplot import imshow,title,show\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "import keras\n",
    "import pydotplus\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "keras.utils.vis_utils.pydot = pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_resnet(input_shape =[256,256,3], dropout_rate=0.2,l2_lambda = 0.0002):\n",
    "    #Encoder\n",
    "    #input = Input(shape = input_shape, name = \"input\")\n",
    "    model0 = keras.applications.resnet50.ResNet50(include_top=False, weights=None, input_tensor=None, input_shape=input_shape, pooling=None)\n",
    "    #model0.layers.pop(176)\n",
    "    #model0.layers.pop(175)\n",
    "    \n",
    "    out12 = model0.layers[12]\n",
    "    out38 = model0.layers[38]\n",
    "    out102 = model0.layers[102]\n",
    "    output145 = model0.layers[145]\n",
    "    \n",
    "    activation12 = out12.output\n",
    "    activation38 = out38.output\n",
    "    activation102 = out102.output\n",
    "    activation145 = output145.output\n",
    "    \n",
    "    \n",
    "    final_activation_output = model0.layers[174]\n",
    "    final_activation = final_activation_output.output\n",
    "    \n",
    "    #Decoder\n",
    "    upconv6 = Conv2DTranspose(512,(2, 2), strides=(2, 2), padding='same')(final_activation)\n",
    "    upconv6 = Dropout(dropout_rate)(upconv6)\n",
    "    upconv6  = keras.layers.UpSampling2D(size=(0.5,0.5))(upconv6)\n",
    "    concat6 = concatenate([activation145, upconv6], name = \"concat6\")\n",
    "    conv6_1 = Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv6_1\")(concat6)\n",
    "    conv6_1 = bn(name = \"conv6_1_bn\")(conv6_1)\n",
    "    conv6_2 = Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv6_2\")(conv6_1)\n",
    "    conv6_2 = bn(name = \"conv6_2_bn\")(conv6_2)\n",
    "\n",
    "    upconv7 = Conv2DTranspose(1024,(2, 2), strides=(2, 2), padding='same')(conv6_2)\n",
    "    upconv7 = Dropout(dropout_rate)(upconv7)\n",
    "    upconv7  = keras.layers.UpSampling2D(size=(1,1))(upconv7)\n",
    "    concat7 = concatenate([activation102, upconv7], name = \"concat7\")\n",
    "    conv7_1 = Conv2D(128, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv7_1\")(concat7)\n",
    "    conv7_1 = bn(name = \"conv7_1_bn\")(conv7_1)\n",
    "    conv7_2 = Conv2D(128, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv7_2\")(conv7_1)\n",
    "    conv7_2 = bn(name = \"conv7_2_bn\")(conv7_2)\n",
    "\n",
    "    upconv8 = Conv2DTranspose(256,(2, 2), strides=(2, 2), padding='same')(conv7_2)\n",
    "    upconv8 = Dropout(dropout_rate)(upconv8)\n",
    "    upconv8  = keras.layers.UpSampling2D(size=(2,2))(upconv8)\n",
    "    concat8 = concatenate([activation38, upconv8], name = \"concat8\")\n",
    "    conv8_1 = Conv2D(64, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv8_1\")(concat8)\n",
    "    conv8_1 = bn(name = \"conv8_1_bn\")(conv8_1)\n",
    "    conv8_2 = Conv2D(64, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv8_2\")(conv8_1)\n",
    "    conv8_2 = bn(name = \"conv8_2_bn\")(conv8_2)\n",
    "\n",
    "    upconv9 = Conv2DTranspose(64,(2, 2), strides=(2, 2), padding='same')(conv8_2)\n",
    "    upconv9 = Dropout(dropout_rate)(upconv9)\n",
    "    upconv9  = keras.layers.UpSampling2D(size=(0.5,0.5))(upconv9)\n",
    "    concat9 = concatenate([activation12, upconv9], name = \"concat9\")\n",
    "    conv9_1 = Conv2D(32, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv9_1\")(concat9)\n",
    "    conv9_1 = bn(name = \"conv9_1_bn\")(conv9_1)\n",
    "    conv9_2 = Conv2D(32, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv9_2\")(conv9_1)\n",
    "    conv9_2 = bn(name = \"conv9_2_bn\")(conv9_2)\n",
    "    dropout = Dropout(dropout_rate)(conv9_2)\n",
    "   \n",
    "    up6 = UpSampling2D(size=(8,8))(concat6)\n",
    "    up7 = UpSampling2D(size=(4,4))(concat7)\n",
    "    up8 = UpSampling2D(size=(2,2))(concat8)\n",
    "    \n",
    "    up6 = UpSampling2D(size=(4,4))(up6)\n",
    "    up7 = UpSampling2D(size=(4,4))(up7)\n",
    "    up8 = keras.layers.UpSampling2D(size=(2,2))(up8)\n",
    "    dropout = UpSampling2D(size=(4,4))(dropout)\n",
    "    \n",
    "    concate1 = concatenate([up6,up7,up8,dropout])\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), padding = \"same\", activation = 'sigmoid', name = \"conv10\")(concate1)\n",
    "\n",
    "\n",
    "    model = Model(model0.input, conv10)\n",
    "    #model = conv10\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = [256,256,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model,to_file='/home/deeptek4/Desktop/resnet_unet.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(model0.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(model1.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('/home/deeptek4/MY_WORK/KAGGLE_COMP1/data/Submission2_Data/Unet/unet_train_channel3.npy')\n",
    "Y_train = np.load('/home/deeptek4/MY_WORK/KAGGLE_COMP1/data/Submission2_Data/Unet/unet_ytrain_for_channel3.npy')\n",
    "X_val = np.load('/home/deeptek4/MY_WORK/KAGGLE_COMP1/data/Submission2_Data/Unet/unet_val_channel3.npy')\n",
    "Y_val = np.load('/home/deeptek4/MY_WORK/KAGGLE_COMP1/data/Submission2_Data/Unet/unet_yval_for_channel3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = X_train/255\n",
    "X_val = X_val/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(true, preds):  \n",
    "    scores  = []\n",
    "    for i in range(len(true)):\n",
    "        im1 = true[i]\n",
    "        im2 = preds[i]\n",
    "        intersection = np.logical_and(im1, im2)\n",
    "        score = 2. * intersection.sum() / (im1.sum() + im2.sum())\n",
    "        scores.append(score)\n",
    "    return np.array(scores).mean()#, scores\n",
    "\n",
    "def dice_score(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f)+K.epsilon())\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "class dice_call(Callback):\n",
    "    def __init__(self,validation_data):\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if(epoch%1==0):\n",
    "            print(\"Calc DICE\")\n",
    "            y_pred_val = np.around(self.model.predict(self.x_val,verbose=1))\n",
    "            try:\n",
    "                dice_val = dice(self.y_val, y_pred_val)\n",
    "            except:\n",
    "                pass\n",
    "            print(\"DICE VAL {0}\".format(dice_val))\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "smooth = 1.0\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f1 = K.flatten(K.round(y_pred))\n",
    "    intersection = K.sum(y_true_f * y_pred_f1)\n",
    "    return  (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f1) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeptek4/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
      "W0812 14:13:49.981523 140108015470400 deprecation.py:506] From /home/deeptek4/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0812 14:13:49.999838 140108015470400 deprecation_wrapper.py:119] From /home/deeptek4/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0812 14:13:51.080318 140108015470400 deprecation_wrapper.py:119] From /home/deeptek4/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0812 14:13:51.092742 140108015470400 deprecation.py:323] From /home/deeptek4/anaconda3/envs/deeptek3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "input_shape = [input_size, input_size, 3]\n",
    "dropout_rate = 0.3\n",
    "l2_lambda = 0.0002\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "model = unet_resnet(input_shape, dropout_rate, l2_lambda)\n",
    "#model1 = u_net(input_shape,dropout_rate,l2_lambda)\n",
    "adam = Adam(lr = 0.0001)\n",
    "model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = [dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [dice_call]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train[0:100],Y_train[0:100], batch_size=1, epochs=10, validation_data=(X_val[0:10], Y_val[0:10]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeptek4/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "model0 = keras.applications.resnet50.ResNet50(include_top=False, weights=None, input_tensor=None, input_shape=input_shape, pooling=None)\n",
    "    #model0.layers.pop(176)\n",
    "    #model0.layers.pop(175)\n",
    "\n",
    "out12 = model0.layers[12]\n",
    "out38 = model0.layers[38]\n",
    "out102 = model0.layers[102]\n",
    "output145 = model0.layers[145]\n",
    "output174 = model0.layers[174]\n",
    "\n",
    "activation12 = out12.output\n",
    "activation38 = out38.output\n",
    "activation102 = out102.output\n",
    "activation145 = output145.output\n",
    "activation174 = output174.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_activation_output = model0.layers[174]\n",
    "final_activation = final_activation_output.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_98/Relu:0' shape=(?, 8, 8, 2048) dtype=float32>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_139/Relu:0' shape=(?, 8, 8, 512) dtype=float32>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#middle \n",
    "\n",
    "middle_block = Conv2D(8, (2, 2), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda))(final_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_150/Relu:0' shape=(?, 64, 64, 64) dtype=float32>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_157/Relu:0' shape=(?, 64, 64, 256) dtype=float32>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_175/Relu:0' shape=(?, 16, 16, 1024) dtype=float32>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_188/Relu:0' shape=(?, 8, 8, 512) dtype=float32>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_2/BiasAdd:0' shape=(?, 8, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_activation  = \n",
    "\n",
    "#final_activation = Conv2D(64, (3, 3), padding = \"same\", activation='relu', kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv5_1\")(final_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.core import Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv6 = Conv2DTranspose(64,(2,2), padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(middle_block)\n",
    "#     upconv6 = Dropout(dropout_rate)(upconv6)\n",
    "#     concat6 = concatenate([conv4_2, upconv6], name = \"concat6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(64)])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upconv6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upconv6 = Dropout(0.2)(upconv6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_transpose_20/BiasAdd:0' shape=(?, ?, ?, 64) dtype=float32>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upconv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    " concat6 = concatenate([activation145, upconv6], name = \"concat6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat6_12/concat:0' shape=(?, 8, 8, 576) dtype=float32>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv6_1 = Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv6_1\")(concat6)\n",
    "conv6_1 = bn(name = \"conv6_1_bn\")(conv6_1)\n",
    "conv6_2 = Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv6_2\")(conv6_1)\n",
    "conv6_2 = bn(name = \"conv6_2_bn\")(conv6_2)\n",
    "conv6_reshape = Reshape((16,16,64))(conv6_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv7 = Conv2DTranspose(128,(3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(conv6_reshape)\n",
    "upconv7 = Dropout(dropout_rate)(upconv7)\n",
    "#upconv7  = keras.layers.UpSampling2D(size=(1,1))(upconv7)\n",
    "concat7 = concatenate([activation102, upconv7], name = \"concat7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(16), Dimension(16), Dimension(128)])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv7_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv7_1 = Conv2D(128, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv7_1\")(concat7)\n",
    "conv7_1 = bn(name = \"conv7_1_bn\")(conv7_1)\n",
    "conv7_2 = Conv2D(128, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv7_2\")(conv7_1)\n",
    "conv7_2 = bn(name = \"conv7_2_bn\")(conv7_2)\n",
    "conv7_reshape = Reshape((64,64,8))(conv7_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv8 = Conv2DTranspose(256,(3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(conv7_reshape)\n",
    "upconv8 = Dropout(dropout_rate)(upconv8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_18/cond/Merge:0' shape=(?, ?, ?, 256) dtype=float32>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upconv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat8 = concatenate([activation38, upconv8], name = \"concat8\")\n",
    "conv8_1 = Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv8_1\")(concat8)\n",
    "conv8_1 = bn(name = \"conv8_1_bn\")(conv8_1)\n",
    "conv8_2 = Conv2D(256, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv8_2\")(conv8_1)\n",
    "conv8_2 = bn(name = \"conv8_2_bn\")(conv8_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv8_2_bn_2/cond/Merge:0' shape=(?, 64, 64, 256) dtype=float32>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv8_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_150/Relu:0' shape=(?, 64, 64, 64) dtype=float32>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv9 = Conv2DTranspose(256,(3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(conv8_2)\n",
    "upconv9 = Dropout(dropout_rate)(upconv9)\n",
    "concat9 = concatenate([activation12, upconv9], name = \"concat9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv9_1 = Conv2D(1024, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv9_1\")(concat9)\n",
    "conv9_1 = bn(name = \"conv9_1_bn\")(conv9_1)\n",
    "conv9_2 = Conv2D(1024, (3, 3), padding = \"same\", kernel_regularizer=regularizers.l2(l2_lambda), name = \"conv9_2\")(conv9_1)\n",
    "conv9_2 = bn(name = \"conv9_2_bn\")(conv9_2)\n",
    "conv9_reshape = Reshape((256,256,64))(conv9_2)\n",
    "\n",
    "conv10 = Conv2D(1, (1, 1), padding = \"same\", activation = 'sigmoid', name = \"conv10\")(conv9_reshape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_3:0\", shape=(?, 256, 256, 3), dtype=float32) at layer \"input_3\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-596f42b4aa7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeptek3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_3:0\", shape=(?, 256, 256, 3), dtype=float32) at layer \"input_3\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "m1 = Model(model0.input,conv10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4194304/65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    " concat7 = concatenate([activation102, upconv7], name = \"concat6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv6_2_bn_2/cond/Merge:0' shape=(?, 8, 8, 256) dtype=float32>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv6_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv6_2_reshape = Reshape((16,16,64))(conv6_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --->  input_3\n",
      "1 --->  conv1_pad\n",
      "2 --->  conv1\n",
      "3 --->  bn_conv1\n",
      "4 --->  activation_99\n",
      "5 --->  pool1_pad\n",
      "6 --->  max_pooling2d_3\n",
      "7 --->  res2a_branch2a\n",
      "8 --->  bn2a_branch2a\n",
      "9 --->  activation_100\n",
      "10 --->  res2a_branch2b\n",
      "11 --->  bn2a_branch2b\n",
      "12 --->  activation_101\n",
      "13 --->  res2a_branch2c\n",
      "14 --->  res2a_branch1\n",
      "15 --->  bn2a_branch2c\n",
      "16 --->  bn2a_branch1\n",
      "17 --->  add_33\n",
      "18 --->  activation_102\n",
      "19 --->  res2b_branch2a\n",
      "20 --->  bn2b_branch2a\n",
      "21 --->  activation_103\n",
      "22 --->  res2b_branch2b\n",
      "23 --->  bn2b_branch2b\n",
      "24 --->  activation_104\n",
      "25 --->  res2b_branch2c\n",
      "26 --->  bn2b_branch2c\n",
      "27 --->  add_34\n",
      "28 --->  activation_105\n",
      "29 --->  res2c_branch2a\n",
      "30 --->  bn2c_branch2a\n",
      "31 --->  activation_106\n",
      "32 --->  res2c_branch2b\n",
      "33 --->  bn2c_branch2b\n",
      "34 --->  activation_107\n",
      "35 --->  res2c_branch2c\n",
      "36 --->  bn2c_branch2c\n",
      "37 --->  add_35\n",
      "38 --->  activation_108\n",
      "39 --->  res3a_branch2a\n",
      "40 --->  bn3a_branch2a\n",
      "41 --->  activation_109\n",
      "42 --->  res3a_branch2b\n",
      "43 --->  bn3a_branch2b\n",
      "44 --->  activation_110\n",
      "45 --->  res3a_branch2c\n",
      "46 --->  res3a_branch1\n",
      "47 --->  bn3a_branch2c\n",
      "48 --->  bn3a_branch1\n",
      "49 --->  add_36\n",
      "50 --->  activation_111\n",
      "51 --->  res3b_branch2a\n",
      "52 --->  bn3b_branch2a\n",
      "53 --->  activation_112\n",
      "54 --->  res3b_branch2b\n",
      "55 --->  bn3b_branch2b\n",
      "56 --->  activation_113\n",
      "57 --->  res3b_branch2c\n",
      "58 --->  bn3b_branch2c\n",
      "59 --->  add_37\n",
      "60 --->  activation_114\n",
      "61 --->  res3c_branch2a\n",
      "62 --->  bn3c_branch2a\n",
      "63 --->  activation_115\n",
      "64 --->  res3c_branch2b\n",
      "65 --->  bn3c_branch2b\n",
      "66 --->  activation_116\n",
      "67 --->  res3c_branch2c\n",
      "68 --->  bn3c_branch2c\n",
      "69 --->  add_38\n",
      "70 --->  activation_117\n",
      "71 --->  res3d_branch2a\n",
      "72 --->  bn3d_branch2a\n",
      "73 --->  activation_118\n",
      "74 --->  res3d_branch2b\n",
      "75 --->  bn3d_branch2b\n",
      "76 --->  activation_119\n",
      "77 --->  res3d_branch2c\n",
      "78 --->  bn3d_branch2c\n",
      "79 --->  add_39\n",
      "80 --->  activation_120\n",
      "81 --->  res4a_branch2a\n",
      "82 --->  bn4a_branch2a\n",
      "83 --->  activation_121\n",
      "84 --->  res4a_branch2b\n",
      "85 --->  bn4a_branch2b\n",
      "86 --->  activation_122\n",
      "87 --->  res4a_branch2c\n",
      "88 --->  res4a_branch1\n",
      "89 --->  bn4a_branch2c\n",
      "90 --->  bn4a_branch1\n",
      "91 --->  add_40\n",
      "92 --->  activation_123\n",
      "93 --->  res4b_branch2a\n",
      "94 --->  bn4b_branch2a\n",
      "95 --->  activation_124\n",
      "96 --->  res4b_branch2b\n",
      "97 --->  bn4b_branch2b\n",
      "98 --->  activation_125\n",
      "99 --->  res4b_branch2c\n",
      "100 --->  bn4b_branch2c\n",
      "101 --->  add_41\n",
      "102 --->  activation_126\n",
      "103 --->  res4c_branch2a\n",
      "104 --->  bn4c_branch2a\n",
      "105 --->  activation_127\n",
      "106 --->  res4c_branch2b\n",
      "107 --->  bn4c_branch2b\n",
      "108 --->  activation_128\n",
      "109 --->  res4c_branch2c\n",
      "110 --->  bn4c_branch2c\n",
      "111 --->  add_42\n",
      "112 --->  activation_129\n",
      "113 --->  res4d_branch2a\n",
      "114 --->  bn4d_branch2a\n",
      "115 --->  activation_130\n",
      "116 --->  res4d_branch2b\n",
      "117 --->  bn4d_branch2b\n",
      "118 --->  activation_131\n",
      "119 --->  res4d_branch2c\n",
      "120 --->  bn4d_branch2c\n",
      "121 --->  add_43\n",
      "122 --->  activation_132\n",
      "123 --->  res4e_branch2a\n",
      "124 --->  bn4e_branch2a\n",
      "125 --->  activation_133\n",
      "126 --->  res4e_branch2b\n",
      "127 --->  bn4e_branch2b\n",
      "128 --->  activation_134\n",
      "129 --->  res4e_branch2c\n",
      "130 --->  bn4e_branch2c\n",
      "131 --->  add_44\n",
      "132 --->  activation_135\n",
      "133 --->  res4f_branch2a\n",
      "134 --->  bn4f_branch2a\n",
      "135 --->  activation_136\n",
      "136 --->  res4f_branch2b\n",
      "137 --->  bn4f_branch2b\n",
      "138 --->  activation_137\n",
      "139 --->  res4f_branch2c\n",
      "140 --->  bn4f_branch2c\n",
      "141 --->  add_45\n",
      "142 --->  activation_138\n",
      "143 --->  res5a_branch2a\n",
      "144 --->  bn5a_branch2a\n",
      "145 --->  activation_139\n",
      "146 --->  res5a_branch2b\n",
      "147 --->  bn5a_branch2b\n",
      "148 --->  activation_140\n",
      "149 --->  res5a_branch2c\n",
      "150 --->  res5a_branch1\n",
      "151 --->  bn5a_branch2c\n",
      "152 --->  bn5a_branch1\n",
      "153 --->  add_46\n",
      "154 --->  activation_141\n",
      "155 --->  res5b_branch2a\n",
      "156 --->  bn5b_branch2a\n",
      "157 --->  activation_142\n",
      "158 --->  res5b_branch2b\n",
      "159 --->  bn5b_branch2b\n",
      "160 --->  activation_143\n",
      "161 --->  res5b_branch2c\n",
      "162 --->  bn5b_branch2c\n",
      "163 --->  add_47\n",
      "164 --->  activation_144\n",
      "165 --->  res5c_branch2a\n",
      "166 --->  bn5c_branch2a\n",
      "167 --->  activation_145\n",
      "168 --->  res5c_branch2b\n",
      "169 --->  bn5c_branch2b\n",
      "170 --->  activation_146\n",
      "171 --->  res5c_branch2c\n",
      "172 --->  bn5c_branch2c\n",
      "173 --->  add_48\n",
      "174 --->  activation_147\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model0.layers):\n",
    "    print(i,'---> ', layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ----> input_3 ---->  (?, 256, 256, 3)\n",
      "1 ----> conv1_pad ---->  (?, 262, 262, 3)\n",
      "2 ----> conv1 ---->  (?, 128, 128, 64)\n",
      "3 ----> bn_conv1 ---->  (?, 128, 128, 64)\n",
      "4 ----> activation_99 ---->  (?, 128, 128, 64)\n",
      "5 ----> pool1_pad ---->  (?, 130, 130, 64)\n",
      "6 ----> max_pooling2d_3 ---->  (?, 64, 64, 64)\n",
      "7 ----> res2a_branch2a ---->  (?, 64, 64, 64)\n",
      "8 ----> bn2a_branch2a ---->  (?, 64, 64, 64)\n",
      "9 ----> activation_100 ---->  (?, 64, 64, 64)\n",
      "10 ----> res2a_branch2b ---->  (?, 64, 64, 64)\n",
      "11 ----> bn2a_branch2b ---->  (?, 64, 64, 64)\n",
      "12 ----> activation_101 ---->  (?, 64, 64, 64)\n",
      "13 ----> res2a_branch2c ---->  (?, 64, 64, 256)\n",
      "14 ----> res2a_branch1 ---->  (?, 64, 64, 256)\n",
      "15 ----> bn2a_branch2c ---->  (?, 64, 64, 256)\n",
      "16 ----> bn2a_branch1 ---->  (?, 64, 64, 256)\n",
      "17 ----> add_33 ---->  (?, 64, 64, 256)\n",
      "18 ----> activation_102 ---->  (?, 64, 64, 256)\n",
      "19 ----> res2b_branch2a ---->  (?, 64, 64, 64)\n",
      "20 ----> bn2b_branch2a ---->  (?, 64, 64, 64)\n",
      "21 ----> activation_103 ---->  (?, 64, 64, 64)\n",
      "22 ----> res2b_branch2b ---->  (?, 64, 64, 64)\n",
      "23 ----> bn2b_branch2b ---->  (?, 64, 64, 64)\n",
      "24 ----> activation_104 ---->  (?, 64, 64, 64)\n",
      "25 ----> res2b_branch2c ---->  (?, 64, 64, 256)\n",
      "26 ----> bn2b_branch2c ---->  (?, 64, 64, 256)\n",
      "27 ----> add_34 ---->  (?, 64, 64, 256)\n",
      "28 ----> activation_105 ---->  (?, 64, 64, 256)\n",
      "29 ----> res2c_branch2a ---->  (?, 64, 64, 64)\n",
      "30 ----> bn2c_branch2a ---->  (?, 64, 64, 64)\n",
      "31 ----> activation_106 ---->  (?, 64, 64, 64)\n",
      "32 ----> res2c_branch2b ---->  (?, 64, 64, 64)\n",
      "33 ----> bn2c_branch2b ---->  (?, 64, 64, 64)\n",
      "34 ----> activation_107 ---->  (?, 64, 64, 64)\n",
      "35 ----> res2c_branch2c ---->  (?, 64, 64, 256)\n",
      "36 ----> bn2c_branch2c ---->  (?, 64, 64, 256)\n",
      "37 ----> add_35 ---->  (?, 64, 64, 256)\n",
      "38 ----> activation_108 ---->  (?, 64, 64, 256)\n",
      "39 ----> res3a_branch2a ---->  (?, 32, 32, 128)\n",
      "40 ----> bn3a_branch2a ---->  (?, 32, 32, 128)\n",
      "41 ----> activation_109 ---->  (?, 32, 32, 128)\n",
      "42 ----> res3a_branch2b ---->  (?, 32, 32, 128)\n",
      "43 ----> bn3a_branch2b ---->  (?, 32, 32, 128)\n",
      "44 ----> activation_110 ---->  (?, 32, 32, 128)\n",
      "45 ----> res3a_branch2c ---->  (?, 32, 32, 512)\n",
      "46 ----> res3a_branch1 ---->  (?, 32, 32, 512)\n",
      "47 ----> bn3a_branch2c ---->  (?, 32, 32, 512)\n",
      "48 ----> bn3a_branch1 ---->  (?, 32, 32, 512)\n",
      "49 ----> add_36 ---->  (?, 32, 32, 512)\n",
      "50 ----> activation_111 ---->  (?, 32, 32, 512)\n",
      "51 ----> res3b_branch2a ---->  (?, 32, 32, 128)\n",
      "52 ----> bn3b_branch2a ---->  (?, 32, 32, 128)\n",
      "53 ----> activation_112 ---->  (?, 32, 32, 128)\n",
      "54 ----> res3b_branch2b ---->  (?, 32, 32, 128)\n",
      "55 ----> bn3b_branch2b ---->  (?, 32, 32, 128)\n",
      "56 ----> activation_113 ---->  (?, 32, 32, 128)\n",
      "57 ----> res3b_branch2c ---->  (?, 32, 32, 512)\n",
      "58 ----> bn3b_branch2c ---->  (?, 32, 32, 512)\n",
      "59 ----> add_37 ---->  (?, 32, 32, 512)\n",
      "60 ----> activation_114 ---->  (?, 32, 32, 512)\n",
      "61 ----> res3c_branch2a ---->  (?, 32, 32, 128)\n",
      "62 ----> bn3c_branch2a ---->  (?, 32, 32, 128)\n",
      "63 ----> activation_115 ---->  (?, 32, 32, 128)\n",
      "64 ----> res3c_branch2b ---->  (?, 32, 32, 128)\n",
      "65 ----> bn3c_branch2b ---->  (?, 32, 32, 128)\n",
      "66 ----> activation_116 ---->  (?, 32, 32, 128)\n",
      "67 ----> res3c_branch2c ---->  (?, 32, 32, 512)\n",
      "68 ----> bn3c_branch2c ---->  (?, 32, 32, 512)\n",
      "69 ----> add_38 ---->  (?, 32, 32, 512)\n",
      "70 ----> activation_117 ---->  (?, 32, 32, 512)\n",
      "71 ----> res3d_branch2a ---->  (?, 32, 32, 128)\n",
      "72 ----> bn3d_branch2a ---->  (?, 32, 32, 128)\n",
      "73 ----> activation_118 ---->  (?, 32, 32, 128)\n",
      "74 ----> res3d_branch2b ---->  (?, 32, 32, 128)\n",
      "75 ----> bn3d_branch2b ---->  (?, 32, 32, 128)\n",
      "76 ----> activation_119 ---->  (?, 32, 32, 128)\n",
      "77 ----> res3d_branch2c ---->  (?, 32, 32, 512)\n",
      "78 ----> bn3d_branch2c ---->  (?, 32, 32, 512)\n",
      "79 ----> add_39 ---->  (?, 32, 32, 512)\n",
      "80 ----> activation_120 ---->  (?, 32, 32, 512)\n",
      "81 ----> res4a_branch2a ---->  (?, 16, 16, 256)\n",
      "82 ----> bn4a_branch2a ---->  (?, 16, 16, 256)\n",
      "83 ----> activation_121 ---->  (?, 16, 16, 256)\n",
      "84 ----> res4a_branch2b ---->  (?, 16, 16, 256)\n",
      "85 ----> bn4a_branch2b ---->  (?, 16, 16, 256)\n",
      "86 ----> activation_122 ---->  (?, 16, 16, 256)\n",
      "87 ----> res4a_branch2c ---->  (?, 16, 16, 1024)\n",
      "88 ----> res4a_branch1 ---->  (?, 16, 16, 1024)\n",
      "89 ----> bn4a_branch2c ---->  (?, 16, 16, 1024)\n",
      "90 ----> bn4a_branch1 ---->  (?, 16, 16, 1024)\n",
      "91 ----> add_40 ---->  (?, 16, 16, 1024)\n",
      "92 ----> activation_123 ---->  (?, 16, 16, 1024)\n",
      "93 ----> res4b_branch2a ---->  (?, 16, 16, 256)\n",
      "94 ----> bn4b_branch2a ---->  (?, 16, 16, 256)\n",
      "95 ----> activation_124 ---->  (?, 16, 16, 256)\n",
      "96 ----> res4b_branch2b ---->  (?, 16, 16, 256)\n",
      "97 ----> bn4b_branch2b ---->  (?, 16, 16, 256)\n",
      "98 ----> activation_125 ---->  (?, 16, 16, 256)\n",
      "99 ----> res4b_branch2c ---->  (?, 16, 16, 1024)\n",
      "100 ----> bn4b_branch2c ---->  (?, 16, 16, 1024)\n",
      "101 ----> add_41 ---->  (?, 16, 16, 1024)\n",
      "102 ----> activation_126 ---->  (?, 16, 16, 1024)\n",
      "103 ----> res4c_branch2a ---->  (?, 16, 16, 256)\n",
      "104 ----> bn4c_branch2a ---->  (?, 16, 16, 256)\n",
      "105 ----> activation_127 ---->  (?, 16, 16, 256)\n",
      "106 ----> res4c_branch2b ---->  (?, 16, 16, 256)\n",
      "107 ----> bn4c_branch2b ---->  (?, 16, 16, 256)\n",
      "108 ----> activation_128 ---->  (?, 16, 16, 256)\n",
      "109 ----> res4c_branch2c ---->  (?, 16, 16, 1024)\n",
      "110 ----> bn4c_branch2c ---->  (?, 16, 16, 1024)\n",
      "111 ----> add_42 ---->  (?, 16, 16, 1024)\n",
      "112 ----> activation_129 ---->  (?, 16, 16, 1024)\n",
      "113 ----> res4d_branch2a ---->  (?, 16, 16, 256)\n",
      "114 ----> bn4d_branch2a ---->  (?, 16, 16, 256)\n",
      "115 ----> activation_130 ---->  (?, 16, 16, 256)\n",
      "116 ----> res4d_branch2b ---->  (?, 16, 16, 256)\n",
      "117 ----> bn4d_branch2b ---->  (?, 16, 16, 256)\n",
      "118 ----> activation_131 ---->  (?, 16, 16, 256)\n",
      "119 ----> res4d_branch2c ---->  (?, 16, 16, 1024)\n",
      "120 ----> bn4d_branch2c ---->  (?, 16, 16, 1024)\n",
      "121 ----> add_43 ---->  (?, 16, 16, 1024)\n",
      "122 ----> activation_132 ---->  (?, 16, 16, 1024)\n",
      "123 ----> res4e_branch2a ---->  (?, 16, 16, 256)\n",
      "124 ----> bn4e_branch2a ---->  (?, 16, 16, 256)\n",
      "125 ----> activation_133 ---->  (?, 16, 16, 256)\n",
      "126 ----> res4e_branch2b ---->  (?, 16, 16, 256)\n",
      "127 ----> bn4e_branch2b ---->  (?, 16, 16, 256)\n",
      "128 ----> activation_134 ---->  (?, 16, 16, 256)\n",
      "129 ----> res4e_branch2c ---->  (?, 16, 16, 1024)\n",
      "130 ----> bn4e_branch2c ---->  (?, 16, 16, 1024)\n",
      "131 ----> add_44 ---->  (?, 16, 16, 1024)\n",
      "132 ----> activation_135 ---->  (?, 16, 16, 1024)\n",
      "133 ----> res4f_branch2a ---->  (?, 16, 16, 256)\n",
      "134 ----> bn4f_branch2a ---->  (?, 16, 16, 256)\n",
      "135 ----> activation_136 ---->  (?, 16, 16, 256)\n",
      "136 ----> res4f_branch2b ---->  (?, 16, 16, 256)\n",
      "137 ----> bn4f_branch2b ---->  (?, 16, 16, 256)\n",
      "138 ----> activation_137 ---->  (?, 16, 16, 256)\n",
      "139 ----> res4f_branch2c ---->  (?, 16, 16, 1024)\n",
      "140 ----> bn4f_branch2c ---->  (?, 16, 16, 1024)\n",
      "141 ----> add_45 ---->  (?, 16, 16, 1024)\n",
      "142 ----> activation_138 ---->  (?, 16, 16, 1024)\n",
      "143 ----> res5a_branch2a ---->  (?, 8, 8, 512)\n",
      "144 ----> bn5a_branch2a ---->  (?, 8, 8, 512)\n",
      "145 ----> activation_139 ---->  (?, 8, 8, 512)\n",
      "146 ----> res5a_branch2b ---->  (?, 8, 8, 512)\n",
      "147 ----> bn5a_branch2b ---->  (?, 8, 8, 512)\n",
      "148 ----> activation_140 ---->  (?, 8, 8, 512)\n",
      "149 ----> res5a_branch2c ---->  (?, 8, 8, 2048)\n",
      "150 ----> res5a_branch1 ---->  (?, 8, 8, 2048)\n",
      "151 ----> bn5a_branch2c ---->  (?, 8, 8, 2048)\n",
      "152 ----> bn5a_branch1 ---->  (?, 8, 8, 2048)\n",
      "153 ----> add_46 ---->  (?, 8, 8, 2048)\n",
      "154 ----> activation_141 ---->  (?, 8, 8, 2048)\n",
      "155 ----> res5b_branch2a ---->  (?, 8, 8, 512)\n",
      "156 ----> bn5b_branch2a ---->  (?, 8, 8, 512)\n",
      "157 ----> activation_142 ---->  (?, 8, 8, 512)\n",
      "158 ----> res5b_branch2b ---->  (?, 8, 8, 512)\n",
      "159 ----> bn5b_branch2b ---->  (?, 8, 8, 512)\n",
      "160 ----> activation_143 ---->  (?, 8, 8, 512)\n",
      "161 ----> res5b_branch2c ---->  (?, 8, 8, 2048)\n",
      "162 ----> bn5b_branch2c ---->  (?, 8, 8, 2048)\n",
      "163 ----> add_47 ---->  (?, 8, 8, 2048)\n",
      "164 ----> activation_144 ---->  (?, 8, 8, 2048)\n",
      "165 ----> res5c_branch2a ---->  (?, 8, 8, 512)\n",
      "166 ----> bn5c_branch2a ---->  (?, 8, 8, 512)\n",
      "167 ----> activation_145 ---->  (?, 8, 8, 512)\n",
      "168 ----> res5c_branch2b ---->  (?, 8, 8, 512)\n",
      "169 ----> bn5c_branch2b ---->  (?, 8, 8, 512)\n",
      "170 ----> activation_146 ---->  (?, 8, 8, 512)\n",
      "171 ----> res5c_branch2c ---->  (?, 8, 8, 2048)\n",
      "172 ----> bn5c_branch2c ---->  (?, 8, 8, 2048)\n",
      "173 ----> add_48 ---->  (?, 8, 8, 2048)\n",
      "174 ----> activation_147 ---->  (?, 8, 8, 2048)\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model0.layers):\n",
    "    print(i, '---->', layer.name ,'----> ', layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
